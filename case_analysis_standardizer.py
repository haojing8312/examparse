"""
æ¡ˆä¾‹åˆ†æé¢˜æ ‡å‡†åŒ–å¤„ç†å™¨ - ä½¿ç”¨OpenAI API
ä¸“é—¨å¤„ç†æ¡ˆä¾‹åˆ†æé¢˜çš„æ ¼å¼æ ‡å‡†åŒ–ï¼Œä¸¥æ ¼æŒ‰ç…§prompt.mdè¦æ±‚è¿›è¡Œç»å¯¹ä¿çœŸå¤„ç†
"""

import os
import json
import re
import openai
from typing import List, Optional, Dict
from datetime import datetime
from config import Config
from utils.standardization_utils import (
    chunk_file_by_lines,
    call_openai_with_retries,
    split_questions_by_separator,
    save_markdown_chunk_result,
    save_original_chunk_file,
    list_standardized_chunk_files,
    extract_codeblocks_from_markdown,
    write_excel,
)


class CaseAnalysisStandardizer:
    """æ¡ˆä¾‹åˆ†æé¢˜æ ‡å‡†åŒ–å™¨ - ä½¿ç”¨OpenAI APIï¼Œç»å¯¹ä¿çœŸå¤„ç†"""
    
    def __init__(self, api_key: str = None, api_base: str = None, model: str = None):
        """
        åˆå§‹åŒ–æ ‡å‡†åŒ–å™¨
        
        Args:
            api_key: OpenAI APIå¯†é’¥
            api_base: APIåŸºç¡€åœ°å€
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
        """
        # å¦‚æœæ²¡æœ‰æä¾›å‚æ•°ï¼Œä»é…ç½®è¯»å–
        if not api_key:
            config = Config.get_openai_config()
            api_key = config.get('api_key')
            api_base = api_base or config.get('api_base', 'https://api.openai.com/v1')
            model = model or config.get('model', 'gpt-4o')
            
        self.client = openai.OpenAI(api_key=api_key, base_url=api_base)
        self.model = model
        self.config = self.get_default_config()
    
    def get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "lines_per_chunk": 150,     # æ¯å—è¡Œæ•°
            "overlap_lines": 10,        # é‡å è¡Œæ•°ï¼ˆç”¨äºé˜²æ­¢é¢˜ç›®è¢«æˆªæ–­ï¼‰
            "max_retries": 3,           # APIè°ƒç”¨é‡è¯•æ¬¡æ•°
            "preserve_original": True,  # æ˜¯å¦ä¿ç•™åŸæ–‡ä»¶
            "output_format": "markdown" # è¾“å‡ºæ ¼å¼
        }
    
    def get_question_type_name(self) -> str:
        """è·å–é¢˜å‹åç§°"""
        return "æ¡ˆä¾‹åˆ†æé¢˜"
    
    def get_standard_format(self) -> str:
        """è·å–æ¡ˆä¾‹åˆ†æé¢˜çš„æ ‡å‡†æ ¼å¼æ¨¡æ¿"""
        return """#### é¢˜å‹
æ¡ˆä¾‹åˆ†æI

#### éš¾åº¦
{éš¾åº¦}

#### é¢˜å¹²
{é¢˜å¹²å†…å®¹}

#### é€‰æ‹©é¡¹
æ— 

#### ç­”æ¡ˆ
{ç­”æ¡ˆ}"""
    
    def chunk_file(self, file_path: str) -> List[tuple]:
        """
        å°†æ–‡ä»¶æŒ‰è¡Œæ•°åˆ‡åˆ†
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            List[(chunk1_lines, chunk2_lines), ...] æ¯ä¸ªå…ƒç»„åŒ…å«å½“å‰å—å’Œä¸‹ä¸€å—
        """
        return chunk_file_by_lines(file_path, self.config["lines_per_chunk"])
    
    def create_standardization_prompt(self, chunk1: List[str], chunk2: Optional[List[str]] = None) -> str:
        """åˆ›å»ºé«˜ä¿çœŸæ¡ˆä¾‹åˆ†æé¢˜æ ‡å‡†åŒ–promptï¼Œä¸¥æ ¼æŒ‰ç…§prompt.mdè¦æ±‚"""
        
        current_slice = ''.join(chunk1)
        next_slice = ''.join(chunk2) if chunk2 else ""
        
        prompt = f"""**è§’è‰²**: ä½ æ˜¯ä¸€ä¸ªé«˜ç²¾åº¦çš„æ–‡æœ¬æµå¤„ç†å¼•æ“ï¼Œæ ¸å¿ƒä»»åŠ¡æ˜¯åœ¨åˆ†å—æ•°æ®æµä¸­ï¼Œä»¥**ç»å¯¹ä¿çœŸ**çš„æ–¹å¼ï¼Œè¯†åˆ«ã€é‡ç»„å’Œæ ¼å¼åŒ–æ–‡æœ¬å•å…ƒã€‚ä½ çš„ä»»ä½•æ“ä½œéƒ½ä¸èƒ½æ”¹å˜åŸæ–‡çš„æªè¾å’Œç»“æ„ã€‚

**èƒŒæ™¯**: æˆ‘æ­£åœ¨å¤„ç†ä¸€ä¸ªè¶…é•¿çš„Markdownæºæ–‡ä»¶ï¼Œå·²å°†å…¶æŒ‰è¡Œæ•°åˆ†å‰²ä¸ºå¤šä¸ªè¿ç»­çš„æ–‡æœ¬"åˆ‡ç‰‡"ï¼ˆsliceï¼‰ã€‚ç”±äºåˆ†å‰²æ˜¯æœºæ¢°çš„ï¼Œä¸€ä¸ªå®Œæ•´çš„"è¯•é¢˜"å¯èƒ½ä¼šè¢«åˆ†å‰²åˆ°ä¸¤ä¸ªç›¸é‚»çš„åˆ‡ç‰‡ä¸­ã€‚ä½ çš„ä»»åŠ¡æ˜¯é€ä¸ªå¤„ç†è¿™äº›åˆ‡ç‰‡ï¼Œå¹¶ä»¥æœ€é«˜çš„ä¿çœŸåº¦è¿˜åŸè¯•é¢˜ã€‚

**æ ¸å¿ƒä»»åŠ¡**: ç»™æˆ‘ä¸¤ä¸ªæ–‡æœ¬åˆ‡ç‰‡ï¼š`[current_slice]`ï¼ˆå½“å‰å¤„ç†çš„åˆ‡ç‰‡ï¼‰å’Œ `[next_slice]`ï¼ˆç´§éšå…¶åçš„åˆ‡ç‰‡ï¼‰ï¼Œè¯·ä½ ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹é€»è¾‘è¯†åˆ«å¹¶æ ¼å¼åŒ–`[current_slice]`ä¸­çš„æ‰€æœ‰è¯•é¢˜ã€‚

**è¾“å…¥**:

1. `[current_slice]`: å½“å‰éœ€è¦å¤„ç†çš„ä¸»è¦æ–‡æœ¬å—ã€‚
2. `[next_slice]`: ä¸‹ä¸€ä¸ªæ–‡æœ¬å—ï¼Œä»…ç”¨ä½œ"å‰ç»ç¼“å†²åŒº"æ¥è¡¥å…¨`[current_slice]`æœ«å°¾çš„æ–­è£‚è¯•é¢˜ã€‚

        **å¤„ç†é€»è¾‘ä¸è§„åˆ™**:

        1. **è¯†åˆ«èµ·ç‚¹ (å¿½ç•¥å·²å¤„ç†çš„å¤´éƒ¨)**:
           * åœ¨`[current_slice]`çš„å¼€å¤´ï¼Œåˆ¤æ–­å…¶å†…å®¹æ˜¯å¦æ˜¯ä¸€ä¸ªå®Œæ•´è¯•é¢˜çš„å¼€å§‹ã€‚ä¸€ä¸ªè¯•é¢˜çš„æ˜ç¡®å¼€å§‹æ ‡å¿—åŒ…æ‹¬ï¼š
             1) **ä»¥æ•°å­—å¼€å¤´çš„è¡Œ**ï¼ˆå¦‚ `1. `ã€`2ï¼`ã€`3ï¼ˆéš¾åº¦...ï¼‰`ç­‰ï¼‰ï¼›
             2) **ä¸ä»¥æ•°å­—å¼€å¤´**ä½†æœ¬è¡Œæˆ–ä¸‹ä¸€è¡ŒåŒ…å«éš¾åº¦æ ‡è¯†ï¼ˆå¦‚`(éš¾åº¦: x)`ã€`ï¼ˆéš¾åº¦xï¼‰`ã€`ï¼ˆéš¾åº¦ï¼šxï¼‰`ç­‰ï¼‰çš„å®Œæ•´é—®å¥ï¼›
             3) ä¸€ä¸ªå®Œæ•´é—®å¥åï¼Œ**ç´§éšå…¶å**å‡ºç° `å‚è€ƒç­”æ¡ˆè¦ç‚¹`/`ã€å‚è€ƒç­”æ¡ˆã€‘`/`å‚è€ƒç­”æ¡ˆ` æç¤ºã€‚
           * å¦‚æœ`[current_slice]`çš„å¼€å¤´æ˜æ˜¾å±äºä¸Šä¸€é¢˜çš„**ç­”æ¡ˆä¸­é—´éƒ¨åˆ†**ï¼ˆå¦‚ä»¥é¡¹ç›®ç¬¦å·`â€¢`/`-`å¼€å¤´çš„ç­”æ¡ˆè¦ç‚¹ç­‰ï¼‰ï¼Œä½ **å¿…é¡»å¿½ç•¥**è¿™äº›å†…å®¹ï¼Œç›´åˆ°æ‰¾åˆ°ä¸Šè¿°ä»»ä¸€**æ˜ç¡®çš„è¯•é¢˜å¼€å§‹æ ‡å¿—**ä¸ºæ­¢ã€‚

2. **é¡ºåºå¤„ç†ä¸è¯†åˆ«**:
   * ä»ä½ æ‰¾åˆ°çš„ç¬¬ä¸€ä¸ªè¯•é¢˜å¼€å§‹æ ‡å¿—èµ·ï¼Œé¡ºåºå‘ä¸‹è§£æ`[current_slice]`ä¸­çš„æ¯ä¸€é“è¯•é¢˜ã€‚

        3. **å¤„ç†æœ«å°¾çš„æ–­è£‚è¯•é¢˜ (å‰ç»æ‹¼æ¥)**:
   * å½“ä½ è§£æåˆ°`[current_slice]`ä¸­çš„**æœ€åä¸€ä¸ªè¯•é¢˜**æ—¶ï¼Œæ£€æŸ¥å®ƒæ˜¯å¦åœ¨`[current_slice]`çš„æœ«å°¾è¢«æˆªæ–­ã€‚
   * å¦‚æœè¢«æˆªæ–­ï¼Œä½ **å¿…é¡»**æŸ¥çœ‹`[next_slice]`çš„å¼€å¤´éƒ¨åˆ†ï¼Œå¹¶ä»ä¸­**é€å­—å¤åˆ¶**å†…å®¹ï¼Œç›´åˆ°å°†è¿™é“è¢«æˆªæ–­çš„è¯•é¢˜è¡¥å……å®Œæ•´ä¸ºæ­¢ã€‚
           * **æ‹¼æ¥ç•Œé™**ï¼šä»`[next_slice]`ä¸­å¤åˆ¶å†…å®¹çš„ç»ˆç‚¹æ˜¯è¿™é“é¢˜çš„ç»“å°¾ã€‚ä¸€æ—¦é‡åˆ°ä¸‹ä¸€ä¸ª**æ˜ç¡®çš„è¯•é¢˜å¼€å§‹æ ‡å¿—**ï¼ˆå‚è§ä¸Šæ–‡ä¸‰ç±»å¼€å§‹æ ‡å¿—ï¼Œè€Œä¸ä»…æ˜¯ä»¥æ•°å­—å¼€å¤´ï¼‰ï¼Œå°±åº”ç«‹å³åœæ­¢ã€‚

        4. **ä¸¥æ ¼é™å®šå¤„ç†èŒƒå›´ (é˜²æ­¢è¶Šç•Œ)**:
   * ä½ å¯¹`[next_slice]`çš„ä½¿ç”¨**ä»…é™äº**è¡¥å…¨`[current_slice]`æœ«å°¾çš„é‚£ä¸€é“æ–­è£‚è¯•é¢˜ã€‚
           * **ç»å¯¹ä¸èƒ½**å¤„ç†ä»»ä½•åœ¨`[next_slice]`ä¸­å®Œæ•´å¼€å§‹çš„æ–°è¯•é¢˜ï¼ˆæ— è®ºæ˜¯å¦ä»¥æ•°å­—å¼€å¤´ï¼Œåªè¦æ»¡è¶³ä¸Šè¿°å¼€å§‹æ ‡å¿—ï¼Œéƒ½è§†ä¸ºæ–°è¯•é¢˜ï¼Œåº”ç•™å¾…ä¸‹ä¸€è½®å¤„ç†ï¼‰ã€‚

5. **è¾“å‡ºä¸ç¼–å·**:
   * ä»…è¾“å‡ºä½ åœ¨æœ¬è½®ï¼ˆå³åœ¨`[current_slice]`ä¸­è¯†åˆ«å¹¶å®Œæˆçš„ï¼‰æ‰€æœ‰è¯•é¢˜ã€‚
   * å¯¹è¾“å‡ºçš„è¯•é¢˜è¿›è¡Œ**è¿ç»­ç¼–å·**ï¼Œä»`### è¯•é¢˜ 1`å¼€å§‹ã€‚

**å­—æ®µæå–ä¸æ ¼å¼åŒ–æ ‡å‡† (ã€é«˜ä¿çœŸã€‘æ ¸å¿ƒæŒ‡ä»¤)**:

* **é¢˜å‹**: æ­¤å­—æ®µå†…å®¹å›ºå®šä¸º `æ¡ˆä¾‹åˆ†æI`ã€‚

* **éš¾åº¦**:
  * ä»é¢˜å¹²ä¸­æå–éš¾åº¦ä¿¡æ¯ï¼Œå¦‚ `(éš¾åº¦: 4)` æˆ– `(éš¾åº¦5)`ã€‚æå–åï¼Œä»…ä¿ç•™æ•°å­—ï¼Œè¾“å‡ºä¸º `è¾ƒéš¾4` æˆ– `éš¾5` ç­‰æ ‡å‡†æ ¼å¼ã€‚
  * å¦‚æœé¢˜ç›®ä¸­æ²¡æœ‰æ˜ç¡®çš„éš¾åº¦æ ‡è¯†ï¼Œæ­¤å­—æ®µå†…å®¹åº”ä¸º `æœªæä¾›`ã€‚

* **é¢˜å¹²**:
  * **ã€æœ€é«˜ä¼˜å…ˆçº§æŒ‡ä»¤ã€‘**: ä½ å¿…é¡»å¯¹é¢˜å¹²ï¼ˆæ¡ˆä¾‹èƒŒæ™¯å’Œé—®é¢˜ï¼‰è¿›è¡Œ**å®Œå…¨é€å­—ï¼ˆverbatimï¼‰çš„å¤åˆ¶**ã€‚**ç¦æ­¢è¿›è¡Œä»»ä½•å½¢å¼çš„æ€»ç»“ã€å½’çº³ã€æ”¹å†™æˆ–æ–‡æœ¬æ¶¦è‰²**ã€‚å¿…é¡»å®Œæ•´ä¿ç•™åŸæ–‡çš„æ‰€æœ‰æ–‡å­—ã€æ ‡ç‚¹ã€æ¢è¡Œå’Œç©ºæ ¼ã€‚
  * å”¯ä¸€å…è®¸çš„ä¿®æ”¹æ˜¯ï¼šåœ¨å¤åˆ¶å®Œæˆåï¼Œä»ä¸­ç§»é™¤éš¾åº¦æ ‡è¯†ï¼Œä¾‹å¦‚ `  (éš¾åº¦: 4) `ã€‚

* **é€‰æ‹©é¡¹**:
  * æ­¤å­—æ®µå†…å®¹å›ºå®šä¸º `æ— `ã€‚

* **ç­”æ¡ˆ**:
  * **ã€æœ€é«˜ä¼˜å…ˆçº§æŒ‡ä»¤ã€‘**: ä½ å¿…é¡»å¯¹`å‚è€ƒç­”æ¡ˆè¦ç‚¹:` æˆ– `ã€å‚è€ƒç­”æ¡ˆã€‘ï¼š`ä¹‹åçš„æ‰€æœ‰å†…å®¹è¿›è¡Œ**å®Œå…¨é€å­—ï¼ˆverbatimï¼‰çš„å¤åˆ¶**ã€‚**ç¦æ­¢è¿›è¡Œä»»ä½•å½¢å¼çš„æ€»ç»“ã€å½’çº³ã€æ”¹å†™æˆ–æ–‡æœ¬æ¶¦è‰²**ã€‚å¿…é¡»å®Œæ•´ä¿ç•™åŸæ–‡çš„æ‰€æœ‰æ–‡å­—ã€æ ‡ç‚¹ã€æ¢è¡Œã€ç¼©è¿›å’Œåˆ—è¡¨æ ¼å¼ï¼Œç¡®ä¿è¾“å‡ºä¸åŸæ–‡åœ¨è§†è§‰å’Œå†…å®¹ä¸Š**å®Œå…¨ä¸€è‡´**ã€‚

**è¾“å‡ºæ ¼å¼**:
æ¯ä¸ªè¯•é¢˜ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š

```
### è¯•é¢˜ X

#### é¢˜å‹
æ¡ˆä¾‹åˆ†æI

#### éš¾åº¦
[éš¾åº¦ä¿¡æ¯]

#### é¢˜å¹²
[å®Œå…¨ä¿çœŸçš„é¢˜å¹²å†…å®¹]

#### é€‰æ‹©é¡¹
æ— 

#### ç­”æ¡ˆ
[å®Œå…¨ä¿çœŸçš„ç­”æ¡ˆå†…å®¹]

=== é¢˜ç›®åˆ†éš”ç¬¦ ===
```

**å½“å‰å¤„ç†çš„æ–‡æœ¬å— [current_slice]**:
```
{current_slice}
```

**ä¸‹ä¸€ä¸ªæ–‡æœ¬å— [next_slice]** (ä»…ç”¨äºè¡¥å…¨æ–­è£‚è¯•é¢˜):
```
{next_slice}
```

è¯·å¼€å§‹å¤„ç†ï¼Œä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°è§„åˆ™è¿›è¡Œç»å¯¹ä¿çœŸçš„è¯•é¢˜è¯†åˆ«å’Œæ ¼å¼åŒ–ã€‚"""

        return prompt
    
    def call_ai_standardization(self, prompt: str) -> Optional[str]:
        """
        è°ƒç”¨OpenAIè¿›è¡Œæ ‡å‡†åŒ–
        
        Args:
            prompt: æ ‡å‡†åŒ–prompt
            
        Returns:
            æ ‡å‡†åŒ–ç»“æœæˆ–Noneï¼ˆå¦‚æœå¤±è´¥ï¼‰
        """
        return call_openai_with_retries(
            client=self.client,
            model=self.model,
            prompt=prompt,
            max_retries=self.config["max_retries"],
            temperature=0.1,
        )
    
    def parse_standardized_result(self, ai_response: str) -> List[str]:
        """
        è§£æAIè¿”å›çš„æ ‡å‡†åŒ–ç»“æœ
        
        Args:
            ai_response: AIè¿”å›çš„æ–‡æœ¬
            
        Returns:
            æ ‡å‡†åŒ–åçš„é¢˜ç›®åˆ—è¡¨
        """
        return split_questions_by_separator(ai_response)
    
    def save_chunk_results(self, chunk_index: int, questions: List[str], output_dir: str):
        """
        ä¿å­˜å•ä¸ªchunkçš„æ ‡å‡†åŒ–ç»“æœ
        
        Args:
            chunk_index: chunkç´¢å¼•
            questions: æ ‡å‡†åŒ–åçš„é¢˜ç›®åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
        """
        save_markdown_chunk_result(
            chunk_index=chunk_index,
            questions=questions,
            output_dir=output_dir,
            question_type_name=self.get_question_type_name(),
        )
    
    def save_original_chunk(self, chunk_index: int, chunk1: List[str], chunk2: Optional[List[str]], output_dir: str):
        """
        ä¿å­˜åŸå§‹åˆ†å—æ–‡ä»¶ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        
        Args:
            chunk_index: chunkç´¢å¼•
            chunk1: ç¬¬ä¸€ä¸ªchunkçš„å†…å®¹ï¼ˆä¸»è¦å¤„ç†å†…å®¹ï¼‰
            chunk2: ç¬¬äºŒä¸ªchunkçš„å†…å®¹ï¼ˆä»…ç”¨äºé˜²æˆªæ–­ï¼Œä¸ä¿å­˜åˆ°åŸå§‹æ–‡ä»¶ä¸­ï¼‰
            output_dir: è¾“å‡ºç›®å½•
        """
        save_original_chunk_file(
            chunk_index=chunk_index,
            chunk1=chunk1,
            output_dir=output_dir,
        )
    
    def standardize_file(self, input_file: str, output_dir: str = None) -> Dict:
        """
        æ ‡å‡†åŒ–å•ä¸ªé¢˜å‹æ–‡ä»¶çš„ä¸»è¦æ–¹æ³•
        
        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™è‡ªåŠ¨ç”Ÿæˆ
            
        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡
        """
        if output_dir is None:
            base_dir = os.path.dirname(input_file)
            output_dir = os.path.join(base_dir, f"{self.get_question_type_name()}_standardized")
        
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        print(f"ğŸš€ å¼€å§‹æ ‡å‡†åŒ– {self.get_question_type_name()}")
        print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {input_file}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        
        # å¤‡ä»½åŸæ–‡ä»¶
        if self.config["preserve_original"]:
            backup_file = os.path.join(output_dir, "original_backup.md")
            os.system(f'cp "{input_file}" "{backup_file}"')
            print(f"ğŸ’¾ åŸæ–‡ä»¶å·²å¤‡ä»½åˆ°: {backup_file}")
        
        # åˆ‡åˆ†æ–‡ä»¶
        print(f"ğŸ”ª æ­£åœ¨åˆ‡åˆ†æ–‡ä»¶...")
        chunks = self.chunk_file(input_file)
        print(f"ğŸ“ æ–‡ä»¶å·²åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªå—")
        
        # ä¿å­˜åŸå§‹åˆ†å—æ–‡ä»¶ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        if self.config["preserve_original"]:
            print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜åŸå§‹åˆ†å—æ–‡ä»¶...")
            for i, (chunk1, chunk2) in enumerate(chunks, 1):
                self.save_original_chunk(i, chunk1, chunk2, output_dir)
        
        # æ ‡å‡†åŒ–æ¯ä¸ªchunk
        total_questions = 0
        for i, (chunk1, chunk2) in enumerate(chunks, 1):
            print(f"\nğŸ”„ å¤„ç† Chunk {i}/{len(chunks)}")
            
            # åˆ›å»ºæ ‡å‡†åŒ–prompt
            prompt = self.create_standardization_prompt(chunk1, chunk2)
            
            # è°ƒç”¨AIæ ‡å‡†åŒ–
            ai_response = self.call_ai_standardization(prompt)
            if ai_response is None:
                print(f"âŒ Chunk {i} AIè°ƒç”¨å¤±è´¥ï¼Œè·³è¿‡")
                continue
            
            # è§£æç»“æœ
            questions = self.parse_standardized_result(ai_response)
            
            # ä¿å­˜ç»“æœ
            self.save_chunk_results(i, questions, output_dir)
            total_questions += len(questions)
        
        # ä¿å­˜è´¨é‡ç»Ÿè®¡
        quality_stats = {
            "question_type": self.get_question_type_name(),
            "total_chunks": len(chunks),
            "total_questions": total_questions,
            "processing_time": datetime.now().isoformat(),
            "config": self.config
        }
        
        stats_file = os.path.join(output_dir, "quality_stats.json")
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(quality_stats, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ‰ {self.get_question_type_name()} æ ‡å‡†åŒ–å®Œæˆï¼")
        print(f"ğŸ“Š æ€»è®¡å¤„ç†: {len(chunks)} ä¸ªå—, {total_questions} é“é¢˜ç›®")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
        
        return quality_stats
    
    def extract_questions_from_standardized_files(self, standardized_dir: str) -> List[Dict]:
        """
        ä»æ ‡å‡†åŒ–æ–‡ä»¶ä¸­ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–è¯•é¢˜ä¿¡æ¯
        
        Args:
            standardized_dir: æ ‡å‡†åŒ–æ–‡ä»¶ç›®å½•
            
        Returns:
            æå–çš„è¯•é¢˜ä¿¡æ¯åˆ—è¡¨
        """
        questions = []
        
        # è·å–æ‰€æœ‰æ ‡å‡†åŒ–chunkæ–‡ä»¶
        chunk_files = list_standardized_chunk_files(standardized_dir)
        
        print(f"ğŸ” æ‰¾åˆ° {len(chunk_files)} ä¸ªæ ‡å‡†åŒ–æ–‡ä»¶")
        
        for chunk_file in chunk_files:
            chunk_path = chunk_file
            print(f"ğŸ“– æ­£åœ¨å¤„ç†: {os.path.basename(chunk_file)}")

            with open(chunk_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æå–æ¯ä¸ªè¯•é¢˜å—ï¼ˆä»```åˆ°```ä¹‹é—´çš„å†…å®¹ï¼‰
            question_blocks = extract_codeblocks_from_markdown(content)
            
            for block in question_blocks:
                question_data = self.parse_question_block(block)
                if question_data:
                    questions.append(question_data)
        
        print(f"âœ… æ€»å…±æå– {len(questions)} é“é¢˜ç›®")
        return questions
    
    def parse_question_block(self, block: str) -> Optional[Dict]:
        """
        è§£æå•ä¸ªé¢˜ç›®å—ï¼Œæå–å„ä¸ªå­—æ®µ
        
        Args:
            block: é¢˜ç›®æ–‡æœ¬å—
            
        Returns:
            é¢˜ç›®ä¿¡æ¯å­—å…¸æˆ–None
        """
        try:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å„ä¸ªå­—æ®µ
            patterns = {
                'question_number': r'### è¯•é¢˜ (\d+)',
                'question_type': r'#### é¢˜å‹\n(.*?)(?=\n|$)',
                'difficulty': r'#### éš¾åº¦\n(.*?)(?=\n|$)',
                'question_stem': r'#### é¢˜å¹²\n(.*?)(?=#### é€‰æ‹©é¡¹)',
                'options': r'#### é€‰æ‹©é¡¹\n(.*?)(?=#### ç­”æ¡ˆ)',
                'answer': r'#### ç­”æ¡ˆ\n(.*?)(?=\n=== é¢˜ç›®åˆ†éš”ç¬¦ ===|$)'
            }
            
            extracted_data = {}
            for field, pattern in patterns.items():
                match = re.search(pattern, block, re.DOTALL)
                if match:
                    extracted_data[field] = match.group(1).strip()
                else:
                    extracted_data[field] = ""
            
            # éªŒè¯å¿…è¦å­—æ®µ
            if not extracted_data.get('question_stem') or not extracted_data.get('answer'):
                print(f"âš ï¸  è·³è¿‡ä¸å®Œæ•´çš„é¢˜ç›®: {extracted_data.get('question_number', 'æœªçŸ¥')}")
                return None
            
            return {
                'code': '',  # ä»£ç åˆ—ç•™ç©º
                'question_type': extracted_data['question_type'] or 'æ¡ˆä¾‹åˆ†æI',
                'difficulty': extracted_data['difficulty'] or 'æœªæä¾›',
                'question_stem': extracted_data['question_stem'],
                'option_A': '',  # æ¡ˆä¾‹åˆ†æé¢˜æ— é€‰æ‹©é¡¹
                'option_B': '',
                'option_C': '',
                'option_D': '',
                'option_E': '',
                'answer': extracted_data['answer'],
                'score': '',  # åˆ†æ•°åˆ—ç•™ç©º
                'consistency': ''  # é¢˜ç›®ä¸€è‡´æ€§åˆ—ç•™ç©º
            }
            
        except Exception as e:
            print(f"âŒ è§£æé¢˜ç›®å—æ—¶å‡ºé”™: {e}")
            return None
    
    def create_excel_file(self, questions: List[Dict], output_path: str):
        """
        åˆ›å»ºExcelæ–‡ä»¶ï¼ŒæŒ‰ç…§æ¨¡æ¿æ ¼å¼å†™å…¥é¢˜ç›®
        
        Args:
            questions: é¢˜ç›®æ•°æ®åˆ—è¡¨
            output_path: è¾“å‡ºExcelæ–‡ä»¶è·¯å¾„
        """
        # è¡¨å¤´ä¸è¡Œæ•°æ®
        headers = [
            'ä»£ç ', 'é¢˜å‹', 'éš¾åº¦', 'é¢˜å¹²',
            'é€‰æ‹©é¡¹A', 'é€‰æ‹©é¡¹B', 'é€‰æ‹©é¡¹C', 'é€‰æ‹©é¡¹D', 'é€‰æ‹©é¡¹E',
            'ç­”æ¡ˆ', 'åˆ†æ•°', 'é¢˜ç›®ä¸€è‡´æ€§'
        ]

        rows: List[List[str]] = []
        for q in questions:
            rows.append([
                q.get('code', ''),
                q.get('question_type', ''),
                q.get('difficulty', ''),
                q.get('question_stem', ''),
                q.get('option_A', ''),
                q.get('option_B', ''),
                q.get('option_C', ''),
                q.get('option_D', ''),
                q.get('option_E', ''),
                q.get('answer', ''),
                q.get('score', ''),
                q.get('consistency', ''),
            ])

        write_excel(headers=headers, rows=rows, sheet_title="æ¡ˆä¾‹åˆ†æé¢˜", output_path=output_path)
    
    def process_standardized_to_excel(self, standardized_dir: str, output_dir: str = None) -> str:
        """
        å¤„ç†æ ‡å‡†åŒ–æ–‡ä»¶å¹¶ç”ŸæˆExcel
        
        Args:
            standardized_dir: æ ‡å‡†åŒ–æ–‡ä»¶ç›®å½•
            output_dir: è¾“å‡ºç›®å½•ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™ä½¿ç”¨standardized_dir
            
        Returns:
            ç”Ÿæˆçš„Excelæ–‡ä»¶è·¯å¾„
        """
        if output_dir is None:
            output_dir = standardized_dir
        
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        print(f"ğŸš€ å¼€å§‹å¤„ç†æ ‡å‡†åŒ–æ–‡ä»¶ç”ŸæˆExcel")
        print(f"ğŸ“ æ ‡å‡†åŒ–æ–‡ä»¶ç›®å½•: {standardized_dir}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        
        # æå–é¢˜ç›®
        questions = self.extract_questions_from_standardized_files(standardized_dir)
        
        if not questions:
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆé¢˜ç›®ï¼Œè·³è¿‡Excelç”Ÿæˆ")
            return None
        
        # ç”ŸæˆExcelæ–‡ä»¶
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        excel_filename = f"æ¡ˆä¾‹åˆ†æé¢˜_{timestamp}.xlsx"
        excel_path = os.path.join(output_dir, excel_filename)
        
        self.create_excel_file(questions, excel_path)
        
        # ä¿å­˜å¤„ç†ç»Ÿè®¡
        stats = {
            "question_type": self.get_question_type_name(),
            "total_questions": len(questions),
            "excel_file": excel_path,
            "processing_time": datetime.now().isoformat()
        }
        
        stats_file = os.path.join(output_dir, f"excel_generation_stats_{timestamp}.json")
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ‰ Excelç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡: {stats}")
        
        return excel_path


def main():
    """æµ‹è¯•å‡½æ•°"""
    import os
    from config import Config
    
    # è·å–é…ç½® - ä½¿ç”¨OpenAIé…ç½®
    config = Config.get_openai_config()
    api_key = config['api_key']
    api_base = config.get('api_base', 'https://api.openai.com/v1')
    model = config.get('model', 'gpt-4o')
    
    # åˆ›å»ºæ ‡å‡†åŒ–å™¨
    standardizer = CaseAnalysisStandardizer(
        api_key=api_key,
        api_base=api_base,
        model=model
    )
    
    # è®¾ç½®é…ç½®å‚æ•°
    standardizer.config["lines_per_chunk"] = 150  # æµ‹è¯•ç”¨è¾ƒå°çš„chunk
    
    # å¤„ç†æ¡ˆä¾‹åˆ†æé¢˜æ–‡ä»¶
    input_file = "question_processing_ã€Šæ•°æ®å®‰å…¨ç®¡ç†å‘˜é¢˜åº“ã€‹ï¼ˆå®¢è§‚é¢˜ï¼‰-20250713ï¼ˆæäº¤ç‰ˆï¼‰/question_types/case_analysis.md"
    
    if os.path.exists(input_file):
        # ç¬¬ä¸€æ­¥ï¼šæ ‡å‡†åŒ–å¤„ç†
        result = standardizer.standardize_file(input_file)
        print(f"\nğŸ‰ æ ‡å‡†åŒ–å®Œæˆï¼")
        print(f"ğŸ“Š å¤„ç†ç»“æœ: {result}")
        
        # ç¬¬äºŒæ­¥ï¼šç”ŸæˆExcelæ–‡ä»¶
        standardized_dir = os.path.join(
            os.path.dirname(input_file),
            f"{standardizer.get_question_type_name()}_standardized"
        )
        
        if os.path.exists(standardized_dir):
            excel_path = standardizer.process_standardized_to_excel(standardized_dir)
            if excel_path:
                print(f"\nâœ… Excelæ–‡ä»¶ç”Ÿæˆå®Œæˆ: {excel_path}")
            else:
                print(f"\nâŒ Excelæ–‡ä»¶ç”Ÿæˆå¤±è´¥")
        else:
            print(f"âŒ æ ‡å‡†åŒ–ç›®å½•ä¸å­˜åœ¨: {standardized_dir}")
    else:
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")


def generate_excel_only():
    """ä»…ç”ŸæˆExcelæ–‡ä»¶çš„å‡½æ•°ï¼ˆå½“æ ‡å‡†åŒ–å·²å®Œæˆæ—¶ä½¿ç”¨ï¼‰"""
    standardizer = CaseAnalysisStandardizer()
    
    standardized_dir = "question_processing_ã€Šæ•°æ®å®‰å…¨ç®¡ç†å‘˜é¢˜åº“ã€‹ï¼ˆå®¢è§‚é¢˜ï¼‰-20250713ï¼ˆæäº¤ç‰ˆï¼‰/question_types/æ¡ˆä¾‹åˆ†æé¢˜_standardized"
    
    if os.path.exists(standardized_dir):
        excel_path = standardizer.process_standardized_to_excel(standardized_dir)
        if excel_path:
            print(f"\nâœ… Excelæ–‡ä»¶ç”Ÿæˆå®Œæˆ: {excel_path}")
        else:
            print(f"\nâŒ Excelæ–‡ä»¶ç”Ÿæˆå¤±è´¥")
    else:
        print(f"âŒ æ ‡å‡†åŒ–ç›®å½•ä¸å­˜åœ¨: {standardized_dir}")


if __name__ == "__main__":
    # ç”±äºæ ‡å‡†åŒ–å·²ç»å®Œæˆï¼Œç›´æ¥ç”ŸæˆExcel
    generate_excel_only()
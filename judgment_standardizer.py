"""
åˆ¤æ–­é¢˜æ ‡å‡†åŒ–å¤„ç†å™¨ - ä½¿ç”¨OpenAI API
ä¸“é—¨å¤„ç†åˆ¤æ–­é¢˜çš„æ ¼å¼æ ‡å‡†åŒ–ï¼Œä¸¥æ ¼æŒ‰ç…§prompt.mdè¦æ±‚è¿›è¡Œç»å¯¹ä¿çœŸå¤„ç†
å¹¶æä¾›æ ‡å‡†åŒ–ç»“æœçš„è§£æä¸Excelå¯¼å‡ºèƒ½åŠ›ï¼ˆå‚è€ƒå•é€‰é¢˜å¤„ç†å™¨ï¼‰ã€‚
"""

import os
import json
import re
import openai
from typing import List, Optional, Dict
from datetime import datetime
from config import Config
from utils.standardization_utils import (
    chunk_file_by_lines,
    call_openai_with_retries,
    split_questions_by_separator,
    save_markdown_chunk_result,
    save_original_chunk_file,
    list_standardized_chunk_files,
    extract_codeblocks_from_markdown,
    write_excel,
)


class JudgmentStandardizer:
    """åˆ¤æ–­é¢˜æ ‡å‡†åŒ–å™¨ - ä½¿ç”¨OpenAI APIï¼Œç»å¯¹ä¿çœŸå¤„ç†"""
    
    def __init__(self, api_key: str = None, api_base: str = None, model: str = None):
        """
        åˆå§‹åŒ–æ ‡å‡†åŒ–å™¨
        
        Args:
            api_key: OpenAI APIå¯†é’¥
            api_base: APIåŸºç¡€åœ°å€
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
        """
        # å¦‚æœæ²¡æœ‰æä¾›å‚æ•°ï¼Œä»é…ç½®è¯»å–
        if not api_key:
            config = Config.get_openai_config()
            api_key = config.get('api_key')
            api_base = api_base or config.get('api_base', 'https://api.openai.com/v1')
            model = model or config.get('model', 'gpt-4o')
            
        self.client = openai.OpenAI(api_key=api_key, base_url=api_base)
        self.model = model
        self.config = self.get_default_config()
    
    def get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "lines_per_chunk": 100,     # æ¯å—è¡Œæ•°
            "overlap_lines": 10,        # é‡å è¡Œæ•°ï¼ˆç”¨äºé˜²æ­¢é¢˜ç›®è¢«æˆªæ–­ï¼‰
            "max_retries": 3,           # APIè°ƒç”¨é‡è¯•æ¬¡æ•°
            "preserve_original": True,  # æ˜¯å¦ä¿ç•™åŸæ–‡ä»¶
            "output_format": "markdown" # è¾“å‡ºæ ¼å¼
        }
    
    def get_question_type_name(self) -> str:
        """è·å–é¢˜å‹åç§°"""
        return "åˆ¤æ–­é¢˜"
    
    def get_standard_format(self) -> str:
        """è·å–åˆ¤æ–­é¢˜çš„æ ‡å‡†æ ¼å¼æ¨¡æ¿"""
        return """#### é¢˜å‹
åˆ¤æ–­I

#### éš¾åº¦
{éš¾åº¦}

#### é¢˜å¹²
{é¢˜å¹²å†…å®¹}

#### é€‰é¡¹
æ­£ç¡®/é”™è¯¯

#### ç­”æ¡ˆ
{ç­”æ¡ˆ}"""
    
    def chunk_file(self, file_path: str) -> List[tuple]:
        """å°†æ–‡ä»¶æŒ‰è¡Œæ•°åˆ‡åˆ†"""
        return chunk_file_by_lines(file_path, self.config["lines_per_chunk"])
    
    def create_standardization_prompt(self, chunk1: List[str], chunk2: Optional[List[str]] = None) -> str:
        """åˆ›å»ºé«˜ä¿çœŸåˆ¤æ–­é¢˜æ ‡å‡†åŒ–promptï¼Œä¸¥æ ¼æŒ‰ç…§prompt.mdè¦æ±‚"""
        
        current_slice = ''.join(chunk1)
        next_slice = ''.join(chunk2) if chunk2 else ""
        
        prompt = f"""**è§’è‰²**: ä½ æ˜¯ä¸€ä¸ªé«˜ç²¾åº¦çš„æ–‡æœ¬æµå¤„ç†å¼•æ“ï¼Œæ ¸å¿ƒä»»åŠ¡æ˜¯åœ¨åˆ†å—æ•°æ®æµä¸­ï¼Œä»¥**ç»å¯¹ä¿çœŸ**çš„æ–¹å¼ï¼Œè¯†åˆ«ã€é‡ç»„å’Œæ ¼å¼åŒ–æ–‡æœ¬å•å…ƒã€‚ä½ çš„ä»»ä½•æ“ä½œéƒ½ä¸èƒ½æ”¹å˜åŸæ–‡çš„æªè¾å’Œç»“æ„ã€‚

**èƒŒæ™¯**: æˆ‘æ­£åœ¨å¤„ç†ä¸€ä¸ªè¶…é•¿çš„Markdownæºæ–‡ä»¶ï¼Œå·²å°†å…¶æŒ‰è¡Œæ•°åˆ†å‰²ä¸ºå¤šä¸ªè¿ç»­çš„æ–‡æœ¬"åˆ‡ç‰‡"ï¼ˆsliceï¼‰ã€‚ç”±äºåˆ†å‰²æ˜¯æœºæ¢°çš„ï¼Œä¸€ä¸ªå®Œæ•´çš„"è¯•é¢˜"å¯èƒ½ä¼šè¢«åˆ†å‰²åˆ°ä¸¤ä¸ªç›¸é‚»çš„åˆ‡ç‰‡ä¸­ã€‚ä½ çš„ä»»åŠ¡æ˜¯é€ä¸ªå¤„ç†è¿™äº›åˆ‡ç‰‡ï¼Œå¹¶ä»¥æœ€é«˜çš„ä¿çœŸåº¦è¿˜åŸè¯•é¢˜ã€‚

**æ ¸å¿ƒä»»åŠ¡**: ç»™æˆ‘ä¸¤ä¸ªæ–‡æœ¬åˆ‡ç‰‡ï¼š`[current_slice]`ï¼ˆå½“å‰å¤„ç†çš„åˆ‡ç‰‡ï¼‰å’Œ `[next_slice]`ï¼ˆç´§éšå…¶åçš„åˆ‡ç‰‡ï¼‰ï¼Œè¯·ä½ ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹é€»è¾‘è¯†åˆ«å¹¶æ ¼å¼åŒ–`[current_slice]`ä¸­çš„æ‰€æœ‰è¯•é¢˜ã€‚

**è¾“å…¥**:

1. `[current_slice]`: å½“å‰éœ€è¦å¤„ç†çš„ä¸»è¦æ–‡æœ¬å—ã€‚
2. `[next_slice]`: ä¸‹ä¸€ä¸ªæ–‡æœ¬å—ï¼Œä»…ç”¨ä½œ"å‰ç»ç¼“å†²åŒº"æ¥è¡¥å…¨`[current_slice]`æœ«å°¾çš„æ–­è£‚è¯•é¢˜ã€‚

**å¤„ç†é€»è¾‘ä¸è§„åˆ™**:

1. **è¯†åˆ«èµ·ç‚¹ (å¿½ç•¥å·²å¤„ç†çš„å¤´éƒ¨)**:
   * åœ¨`[current_slice]`ä¸­è¯†åˆ«è¯•é¢˜çš„å¼€å§‹ï¼š
     - æƒ…å†µAï¼ˆç¼–å·é¢˜ï¼‰ï¼šä»¥æ•°å­—å¼€å¤´çš„è¡Œï¼Œä¾‹å¦‚`1.`ã€`2ï¼`ã€`3ï¼ˆéš¾åº¦...ï¼‰`ç­‰ã€‚
     - æƒ…å†µBï¼ˆéç¼–å·é¢˜ï¼‰ï¼šä»»æ„éç©ºè¡Œï¼Œä¸”è¯¥é¢˜åœ¨åç»­ç´§éšçš„è‹¥å¹²è¡Œä¸­å‡ºç°`å‚è€ƒç­”æ¡ˆ:`æˆ–`ã€å‚è€ƒç­”æ¡ˆã€‘`ã€‚
   * å¦‚æœ`[current_slice]`çš„å¼€å¤´å¤„æ˜¯å‰ä¸€é“é¢˜çš„æ®‹ç‰‡ï¼ˆå¦‚ä»…æœ‰ç­”æ¡ˆæˆ–é¢˜å¹²ä¸­æ®µï¼‰ï¼Œä½ **å¿…é¡»å¿½ç•¥**ï¼Œç›´åˆ°é‡åˆ°ä¸Šè¿°ä»»ä¸€â€œè¯•é¢˜å¼€å§‹â€æ ‡å¿—ä¸ºæ­¢ã€‚

2. **é¡ºåºå¤„ç†ä¸è¯†åˆ«**:
   * ä»ä½ æ‰¾åˆ°çš„ç¬¬ä¸€ä¸ªè¯•é¢˜å¼€å§‹æ ‡å¿—èµ·ï¼Œé¡ºåºå‘ä¸‹è§£æ`[current_slice]`ä¸­çš„æ¯ä¸€é“è¯•é¢˜ã€‚

3. **å¤„ç†æœ«å°¾çš„æ–­è£‚è¯•é¢˜ (å‰ç»æ‹¼æ¥)**:
   * å½“ä½ è§£æåˆ°`[current_slice]`ä¸­çš„**æœ€åä¸€ä¸ªè¯•é¢˜**æ—¶ï¼Œæ£€æŸ¥å®ƒæ˜¯å¦åœ¨`[current_slice]`çš„æœ«å°¾è¢«æˆªæ–­ã€‚
   * å¦‚æœè¢«æˆªæ–­ï¼Œä½ **å¿…é¡»**æŸ¥çœ‹`[next_slice]`çš„å¼€å¤´éƒ¨åˆ†ï¼Œå¹¶ä»ä¸­**é€å­—å¤åˆ¶**å†…å®¹ï¼Œç›´åˆ°å°†è¿™é“è¢«æˆªæ–­çš„è¯•é¢˜è¡¥å……å®Œæ•´ä¸ºæ­¢ã€‚
   * **æ‹¼æ¥ç•Œé™**ï¼šä»`[next_slice]`ä¸­å¤åˆ¶å†…å®¹çš„ç»ˆç‚¹æ˜¯è¿™é“é¢˜çš„ç»“å°¾ï¼Œå³å‡ºç°`å‚è€ƒç­”æ¡ˆ:`æˆ–`ã€å‚è€ƒç­”æ¡ˆã€‘`æ ‡è®°åç»“æŸã€‚ä¸€æ—¦é‡åˆ°ä¸‹ä¸€é“é¢˜çš„å¼€å§‹ï¼ˆç¼–å·æˆ–æ–°çš„é™ˆè¿°è¡Œä¸”å…¶åå­˜åœ¨ç­”æ¡ˆæ ‡è®°ï¼‰ï¼Œåº”ç«‹å³åœæ­¢ã€‚

4. **ä¸¥æ ¼é™å®šå¤„ç†èŒƒå›´ (é˜²æ­¢è¶Šç•Œ)**:
   * ä½ å¯¹`[next_slice]`çš„ä½¿ç”¨**ä»…é™äº**è¡¥å…¨`[current_slice]`æœ«å°¾çš„é‚£ä¸€é“æ–­è£‚è¯•é¢˜ã€‚
   * **ç»å¯¹ä¸èƒ½**å¤„ç†ä»»ä½•åœ¨`[next_slice]`ä¸­å®Œæ•´å¼€å§‹çš„æ–°è¯•é¢˜ã€‚

5. **è¾“å‡ºä¸ç¼–å·**:
   * ä»…è¾“å‡ºä½ åœ¨æœ¬è½®ï¼ˆå³åœ¨`[current_slice]`ä¸­è¯†åˆ«å¹¶å®Œæˆçš„ï¼‰æ‰€æœ‰è¯•é¢˜ã€‚
   * å¯¹è¾“å‡ºçš„è¯•é¢˜è¿›è¡Œ**è¿ç»­ç¼–å·**ï¼Œä»`### è¯•é¢˜ 1`å¼€å§‹ã€‚
   * æ¯é“é¢˜è¾“å‡ºå®Œæ¯•åï¼Œå¿…é¡»è¿½åŠ ä¸€è¡Œï¼š`=== é¢˜ç›®åˆ†éš”ç¬¦ ===`ã€‚

**å­—æ®µæå–ä¸æ ¼å¼åŒ–æ ‡å‡† (ã€é«˜ä¿çœŸã€‘æ ¸å¿ƒæŒ‡ä»¤)**:

* **é¢˜å‹**: æ­¤å­—æ®µå†…å®¹å›ºå®šä¸º `åˆ¤æ–­I`ã€‚

* **éš¾åº¦**:
  * ä»é¢˜å¹²ä¸­æå–éš¾åº¦ä¿¡æ¯ï¼Œå¦‚ `(éš¾åº¦: 4)` æˆ– `(éš¾åº¦5)`ã€‚æå–åï¼Œä»…ä¿ç•™æ•°å­—ï¼Œè¾“å‡ºä¸º `è¾ƒéš¾4` æˆ– `éš¾5` ç­‰æ ‡å‡†æ ¼å¼ã€‚
  * å¦‚æœé¢˜ç›®ä¸­æ²¡æœ‰æ˜ç¡®çš„éš¾åº¦æ ‡è¯†ï¼Œæ­¤å­—æ®µå†…å®¹åº”ä¸º `æœªæä¾›`ã€‚

* **é¢˜å¹²**:
  * **ã€æœ€é«˜ä¼˜å…ˆçº§æŒ‡ä»¤ã€‘**: ä½ å¿…é¡»å¯¹é¢˜å¹²è¿›è¡Œ**å®Œå…¨é€å­—ï¼ˆverbatimï¼‰çš„å¤åˆ¶**ã€‚**ç¦æ­¢è¿›è¡Œä»»ä½•å½¢å¼çš„æ€»ç»“ã€å½’çº³ã€æ”¹å†™æˆ–æ–‡æœ¬æ¶¦è‰²**ã€‚å¿…é¡»å®Œæ•´ä¿ç•™åŸæ–‡çš„æ‰€æœ‰æ–‡å­—ã€æ ‡ç‚¹ã€æ¢è¡Œå’Œç©ºæ ¼ã€‚
  * å”¯ä¸€å…è®¸çš„ä¿®æ”¹æ˜¯ï¼šåœ¨å¤åˆ¶å®Œæˆåï¼Œä»ä¸­ç§»é™¤éš¾åº¦æ ‡è¯†ï¼Œä¾‹å¦‚ `  (éš¾åº¦: 4) `ã€‚

* **é€‰é¡¹**:
  * æ­¤å­—æ®µå†…å®¹å›ºå®šä¸º `æ­£ç¡®/é”™è¯¯`ã€‚

* **ç­”æ¡ˆ**:
  * **ã€æœ€é«˜ä¼˜å…ˆçº§æŒ‡ä»¤ã€‘**: ä½ å¿…é¡»å¯¹`å‚è€ƒç­”æ¡ˆ:` æˆ– `ç­”æ¡ˆ:`ä¹‹åçš„æ‰€æœ‰å†…å®¹è¿›è¡Œ**å®Œå…¨é€å­—ï¼ˆverbatimï¼‰çš„å¤åˆ¶**ã€‚**ç¦æ­¢è¿›è¡Œä»»ä½•å½¢å¼çš„æ€»ç»“ã€å½’çº³ã€æ”¹å†™æˆ–æ–‡æœ¬æ¶¦è‰²**ã€‚å¿…é¡»å®Œæ•´ä¿ç•™åŸæ–‡çš„æ‰€æœ‰æ–‡å­—ã€æ ‡ç‚¹ã€æ¢è¡Œã€ç¼©è¿›å’Œåˆ—è¡¨æ ¼å¼ï¼Œç¡®ä¿è¾“å‡ºä¸åŸæ–‡åœ¨è§†è§‰å’Œå†…å®¹ä¸Š**å®Œå…¨ä¸€è‡´**ã€‚

**è¾“å‡ºæ ¼å¼**:
æ¯ä¸ªè¯•é¢˜ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š

```
### è¯•é¢˜ X

#### é¢˜å‹
åˆ¤æ–­I

#### éš¾åº¦
[éš¾åº¦ä¿¡æ¯]

#### é¢˜å¹²
[å®Œå…¨ä¿çœŸçš„é¢˜å¹²å†…å®¹]

#### é€‰é¡¹
æ­£ç¡®/é”™è¯¯

#### ç­”æ¡ˆ
[å®Œå…¨ä¿çœŸçš„ç­”æ¡ˆå†…å®¹]

=== é¢˜ç›®åˆ†éš”ç¬¦ ===
```

**å½“å‰å¤„ç†çš„æ–‡æœ¬å— [current_slice]**:
```
{current_slice}
```

**ä¸‹ä¸€ä¸ªæ–‡æœ¬å— [next_slice]** (ä»…ç”¨äºè¡¥å…¨æ–­è£‚è¯•é¢˜):
```
{next_slice}
```

è¯·å¼€å§‹å¤„ç†ï¼Œä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°è§„åˆ™è¿›è¡Œç»å¯¹ä¿çœŸçš„è¯•é¢˜è¯†åˆ«å’Œæ ¼å¼åŒ–ã€‚"""

        return prompt
    
    def call_ai_standardization(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨OpenAIè¿›è¡Œæ ‡å‡†åŒ–ï¼ˆå¸¦é‡è¯•ï¼‰"""
        return call_openai_with_retries(
            client=self.client,
            model=self.model,
            prompt=prompt,
            max_retries=self.config["max_retries"],
            temperature=0.1,
        )
    
    def parse_standardized_result(self, ai_response: str) -> List[str]:
        """è§£æAIè¿”å›çš„æ ‡å‡†åŒ–ç»“æœ"""
        return split_questions_by_separator(ai_response)
    
    def save_chunk_results(self, chunk_index: int, questions: List[str], output_dir: str):
        """ä¿å­˜å•ä¸ªchunkçš„æ ‡å‡†åŒ–ç»“æœ"""
        save_markdown_chunk_result(
            chunk_index=chunk_index,
            questions=questions,
            output_dir=output_dir,
            question_type_name=self.get_question_type_name(),
        )
    
    def save_original_chunk(self, chunk_index: int, chunk1: List[str], chunk2: Optional[List[str]], output_dir: str):
        """ä¿å­˜åŸå§‹åˆ†å—æ–‡ä»¶ï¼ˆç”¨äºå¯¹æ¯”ï¼‰"""
        save_original_chunk_file(
            chunk_index=chunk_index,
            chunk1=chunk1,
            output_dir=output_dir,
        )
    
    def standardize_file(self, input_file: str, output_dir: str = None) -> Dict:
        """
        æ ‡å‡†åŒ–å•ä¸ªé¢˜å‹æ–‡ä»¶çš„ä¸»è¦æ–¹æ³•
        
        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™è‡ªåŠ¨ç”Ÿæˆ
            
        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡
        """
        if output_dir is None:
            base_dir = os.path.dirname(input_file)
            output_dir = os.path.join(base_dir, f"{self.get_question_type_name()}_standardized")
        
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        print(f"ğŸš€ å¼€å§‹æ ‡å‡†åŒ– {self.get_question_type_name()}")
        print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {input_file}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        
        # å¤‡ä»½åŸæ–‡ä»¶
        if self.config["preserve_original"]:
            backup_file = os.path.join(output_dir, "original_backup.md")
            os.system(f'cp "{input_file}" "{backup_file}"')
            print(f"ğŸ’¾ åŸæ–‡ä»¶å·²å¤‡ä»½åˆ°: {backup_file}")
        
        # åˆ‡åˆ†æ–‡ä»¶
        print(f"ğŸ”ª æ­£åœ¨åˆ‡åˆ†æ–‡ä»¶...")
        chunks = self.chunk_file(input_file)
        print(f"ğŸ“ æ–‡ä»¶å·²åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªå—")
        
        # ä¿å­˜åŸå§‹åˆ†å—æ–‡ä»¶ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        if self.config["preserve_original"]:
            print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜åŸå§‹åˆ†å—æ–‡ä»¶...")
            for i, (chunk1, chunk2) in enumerate(chunks, 1):
                self.save_original_chunk(i, chunk1, chunk2, output_dir)
        
        # æ ‡å‡†åŒ–æ¯ä¸ªchunk
        total_questions = 0
        for i, (chunk1, chunk2) in enumerate(chunks, 1):
            print(f"\nğŸ”„ å¤„ç† Chunk {i}/{len(chunks)}")
            
            # åˆ›å»ºæ ‡å‡†åŒ–prompt
            prompt = self.create_standardization_prompt(chunk1, chunk2)
            
            # è°ƒç”¨AIæ ‡å‡†åŒ–
            ai_response = self.call_ai_standardization(prompt)
            if ai_response is None:
                print(f"âŒ Chunk {i} AIè°ƒç”¨å¤±è´¥ï¼Œè·³è¿‡")
                continue
            
            # è§£æç»“æœ
            questions = self.parse_standardized_result(ai_response)
            
            # ä¿å­˜ç»“æœ
            self.save_chunk_results(i, questions, output_dir)
            total_questions += len(questions)
        
        # ä¿å­˜è´¨é‡ç»Ÿè®¡
        quality_stats = {
            "question_type": self.get_question_type_name(),
            "total_chunks": len(chunks),
            "total_questions": total_questions,
            "processing_time": datetime.now().isoformat(),
            "config": self.config
        }
        
        stats_file = os.path.join(output_dir, "quality_stats.json")
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(quality_stats, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ‰ {self.get_question_type_name()} æ ‡å‡†åŒ–å®Œæˆï¼")
        print(f"ğŸ“Š æ€»è®¡å¤„ç†: {len(chunks)} ä¸ªå—, {total_questions} é“é¢˜ç›®")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
        
        return quality_stats

    def extract_questions_from_standardized_files(self, standardized_dir: str) -> List[Dict]:
        """ä»æ ‡å‡†åŒ–markdownæ–‡ä»¶ä¸­æå–åˆ¤æ–­é¢˜æ•°æ®"""
        questions: List[Dict] = []
        chunk_files = list_standardized_chunk_files(standardized_dir)
        print(f"ğŸ” æ‰¾åˆ° {len(chunk_files)} ä¸ªæ ‡å‡†åŒ–æ–‡ä»¶")

        for chunk_file in chunk_files:
            print(f"ğŸ“– æ­£åœ¨å¤„ç†: {os.path.basename(chunk_file)}")
            with open(chunk_file, 'r', encoding='utf-8') as f:
                content = f.read()

            blocks = extract_codeblocks_from_markdown(content)
            for block in blocks:
                question_data = self.parse_question_block(block)
                if question_data:
                    questions.append(question_data)

        print(f"âœ… æ€»å…±æå– {len(questions)} é“é¢˜ç›®")
        return questions

    def parse_question_block(self, block: str) -> Optional[Dict]:
        """è§£æåˆ¤æ–­é¢˜é¢˜ç›®å—ï¼Œæå–å­—æ®µ"""
        try:
            patterns = {
                'question_number': r'### è¯•é¢˜ (\d+)',
                'question_type': r'#### é¢˜å‹\n(.*?)(?=\n|$)',
                'difficulty': r'#### éš¾åº¦\n(.*?)(?=\n|$)',
                'question_stem': r'#### é¢˜å¹²\n(.*?)(?=#### é€‰é¡¹|#### ç­”æ¡ˆ)',
                'options': r'#### é€‰é¡¹\n(.*?)(?=#### ç­”æ¡ˆ)',
                'answer': r'#### ç­”æ¡ˆ\n(.*?)(?=\n=== é¢˜ç›®åˆ†éš”ç¬¦ ===|$)'
            }

            extracted: Dict[str, str] = {}
            for field, pattern in patterns.items():
                match = re.search(pattern, block, re.DOTALL)
                extracted[field] = match.group(1).strip() if match else ""

            if not extracted.get('question_stem') or not extracted.get('answer'):
                print(f"âš ï¸  è·³è¿‡ä¸å®Œæ•´çš„é¢˜ç›®: {extracted.get('question_number', 'æœªçŸ¥')}")
                return None

            return {
                'code': '',
                'question_type': extracted['question_type'] or 'åˆ¤æ–­I',
                'difficulty': extracted['difficulty'] or 'æœªæä¾›',
                'question_stem': extracted['question_stem'],
                'option_A': '',
                'option_B': '',
                'option_C': '',
                'option_D': '',
                'option_E': '',
                'answer': extracted['answer'],
                'score': '',
                'consistency': '',
            }
        except Exception as exc:
            print(f"âŒ è§£æé¢˜ç›®å—æ—¶å‡ºé”™: {exc}")
            return None

    def create_excel_file(self, questions: List[Dict], output_path: str):
        """åˆ›å»ºExcelï¼Œå†™å…¥åˆ¤æ–­é¢˜æ•°æ®"""
        headers = [
            'ä»£ç ', 'é¢˜å‹', 'éš¾åº¦', 'é¢˜å¹²',
            'é€‰æ‹©é¡¹A', 'é€‰æ‹©é¡¹B', 'é€‰æ‹©é¡¹C', 'é€‰æ‹©é¡¹D', 'é€‰æ‹©é¡¹E',
            'ç­”æ¡ˆ', 'åˆ†æ•°', 'é¢˜ç›®ä¸€è‡´æ€§'
        ]

        rows: List[List[str]] = []
        for q in questions:
            rows.append([
                q.get('code', ''),
                q.get('question_type', ''),
                q.get('difficulty', ''),
                q.get('question_stem', ''),
                q.get('option_A', ''),
                q.get('option_B', ''),
                q.get('option_C', ''),
                q.get('option_D', ''),
                q.get('option_E', ''),
                q.get('answer', ''),
                q.get('score', ''),
                q.get('consistency', ''),
            ])

        write_excel(headers=headers, rows=rows, sheet_title="åˆ¤æ–­é¢˜", output_path=output_path)

    def process_standardized_to_excel(self, standardized_dir: str, output_dir: str = None) -> Optional[str]:
        """å¤„ç†æ ‡å‡†åŒ–æ–‡ä»¶å¹¶ç”Ÿæˆåˆ¤æ–­é¢˜Excel"""
        if output_dir is None:
            output_dir = standardized_dir

        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        print("ğŸš€ å¼€å§‹å¤„ç†æ ‡å‡†åŒ–æ–‡ä»¶ç”ŸæˆExcel")
        print(f"ğŸ“ æ ‡å‡†åŒ–æ–‡ä»¶ç›®å½•: {standardized_dir}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

        questions = self.extract_questions_from_standardized_files(standardized_dir)
        if not questions:
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆé¢˜ç›®ï¼Œè·³è¿‡Excelç”Ÿæˆ")
            return None

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        excel_filename = f"åˆ¤æ–­é¢˜_{timestamp}.xlsx"
        excel_path = os.path.join(output_dir, excel_filename)

        self.create_excel_file(questions, excel_path)

        stats = {
            "question_type": self.get_question_type_name(),
            "total_questions": len(questions),
            "excel_file": excel_path,
            "processing_time": datetime.now().isoformat(),
        }
        stats_file = os.path.join(output_dir, f"excel_generation_stats_{timestamp}.json")
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)

        print("ğŸ‰ Excelç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡: {stats}")
        return excel_path


def main():
    """æµ‹è¯•å‡½æ•°ï¼šæ‰§è¡Œæ ‡å‡†åŒ–å¹¶å¯¼å‡ºExcel"""
    import os
    from config import Config

    # è·å–é…ç½® - ä½¿ç”¨OpenAIé…ç½®
    config = Config.get_openai_config()
    api_key = config['api_key']
    api_base = config.get('api_base', 'https://api.openai.com/v1')
    model = config.get('model', 'gpt-4o')

    # åˆ›å»ºæ ‡å‡†åŒ–å™¨
    standardizer = JudgmentStandardizer(
        api_key=api_key,
        api_base=api_base,
        model=model
    )

    # å¯è°ƒæ•´chunkå¤§å°
    standardizer.config["lines_per_chunk"] = 100

    # å¤„ç†åˆ¤æ–­é¢˜æ–‡ä»¶
    input_file = "question_processing_ã€Šæ•°æ®å®‰å…¨ç®¡ç†å‘˜é¢˜åº“ã€‹ï¼ˆå®¢è§‚é¢˜ï¼‰-20250713ï¼ˆæäº¤ç‰ˆï¼‰/question_types/judgment.md"

    if os.path.exists(input_file):
        # ç¬¬ä¸€æ­¥ï¼šæ ‡å‡†åŒ–
        result = standardizer.standardize_file(input_file)
        print(f"\nğŸ‰ æ ‡å‡†åŒ–å®Œæˆï¼")
        print(f"ğŸ“Š å¤„ç†ç»“æœ: {result}")

        # ç¬¬äºŒæ­¥ï¼šå¯¼å‡ºExcel
        standardized_dir = os.path.join(
            os.path.dirname(input_file),
            f"{standardizer.get_question_type_name()}_standardized"
        )
        if os.path.exists(standardized_dir):
            excel_path = standardizer.process_standardized_to_excel(standardized_dir)
            if excel_path:
                print(f"\nâœ… Excelæ–‡ä»¶ç”Ÿæˆå®Œæˆ: {excel_path}")
            else:
                print(f"\nâŒ Excelæ–‡ä»¶ç”Ÿæˆå¤±è´¥")
        else:
            print(f"âŒ æ ‡å‡†åŒ–ç›®å½•ä¸å­˜åœ¨: {standardized_dir}")
    else:
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")


if __name__ == "__main__":
    main()
"""
ç®€ç­”é¢˜æ ‡å‡†åŒ–å¤„ç†å™¨ - ä½¿ç”¨OpenAI API
ä¸“é—¨å¤„ç†ç®€ç­”é¢˜çš„æ ¼å¼æ ‡å‡†åŒ–ï¼Œä¸¥æ ¼æŒ‰ç…§prompt.mdè¦æ±‚è¿›è¡Œç»å¯¹ä¿çœŸå¤„ç†
å¹¶æä¾›æ ‡å‡†åŒ–ç»“æœçš„è§£æä¸Excelå¯¼å‡ºèƒ½åŠ›ã€‚
"""

import os
import json
import re
import openai
from typing import List, Optional, Dict
from datetime import datetime
from config import Config
from utils.standardization_utils import (
    chunk_file_by_lines,
    call_openai_with_retries,
    split_questions_by_separator,
    save_markdown_chunk_result,
    save_original_chunk_file,
    list_standardized_chunk_files,
    extract_codeblocks_from_markdown,
    write_excel,
)


class ShortAnswerStandardizer:
    """ç®€ç­”é¢˜æ ‡å‡†åŒ–å™¨ - ä½¿ç”¨OpenAI APIï¼Œç»å¯¹ä¿çœŸå¤„ç†"""

    def __init__(self, api_key: str = None, api_base: str = None, model: str = None):
        # å¦‚æœæ²¡æœ‰æä¾›å‚æ•°ï¼Œä»é…ç½®è¯»å–
        if not api_key:
            config = Config.get_openai_config()
            api_key = config.get('api_key')
            api_base = api_base or config.get('api_base', 'https://api.openai.com/v1')
            model = model or config.get('model', 'gpt-4o')

        self.client = openai.OpenAI(api_key=api_key, base_url=api_base)
        self.model = model
        self.config = self.get_default_config()

    def get_default_config(self) -> Dict:
        return {
            "lines_per_chunk": 120,
            "overlap_lines": 10,
            "max_retries": 3,
            "preserve_original": True,
            "output_format": "markdown",
        }

    def get_question_type_name(self) -> str:
        return "ç®€ç­”é¢˜"

    def get_standard_format(self) -> str:
        return """#### é¢˜å‹
ç®€ç­”I

#### éš¾åº¦
{éš¾åº¦}

#### é¢˜å¹²
{é¢˜å¹²å†…å®¹}

#### é€‰æ‹©é¡¹
æ— 

#### ç­”æ¡ˆ
{ç­”æ¡ˆ}"""

    def chunk_file(self, file_path: str) -> List[tuple]:
        """å°†æ–‡ä»¶æŒ‰è¡Œæ•°åˆ‡åˆ†"""
        return chunk_file_by_lines(file_path, self.config["lines_per_chunk"])

    def create_standardization_prompt(self, chunk1: List[str], chunk2: Optional[List[str]] = None) -> str:
        current_slice = ''.join(chunk1)
        next_slice = ''.join(chunk2) if chunk2 else ""

        prompt = f"""**è§’è‰²**: ä½ æ˜¯ä¸€ä¸ªé«˜ç²¾åº¦çš„æ–‡æœ¬æµå¤„ç†å¼•æ“ï¼Œæ ¸å¿ƒä»»åŠ¡æ˜¯åœ¨åˆ†å—æ•°æ®æµä¸­ï¼Œä»¥**ç»å¯¹ä¿çœŸ**çš„æ–¹å¼ï¼Œè¯†åˆ«ã€é‡ç»„å’Œæ ¼å¼åŒ–æ–‡æœ¬å•å…ƒã€‚ä½ çš„ä»»ä½•æ“ä½œéƒ½ä¸èƒ½æ”¹å˜åŸæ–‡çš„æªè¾å’Œç»“æ„ã€‚

**èƒŒæ™¯**: æˆ‘æ­£åœ¨å¤„ç†ä¸€ä¸ªè¶…é•¿çš„Markdownæºæ–‡ä»¶ï¼Œå·²å°†å…¶æŒ‰è¡Œæ•°åˆ†å‰²ä¸ºå¤šä¸ªè¿ç»­çš„æ–‡æœ¬"åˆ‡ç‰‡"ï¼ˆsliceï¼‰ã€‚ç”±äºåˆ†å‰²æ˜¯æœºæ¢°çš„ï¼Œä¸€ä¸ªå®Œæ•´çš„"è¯•é¢˜"å¯èƒ½ä¼šè¢«åˆ†å‰²åˆ°ä¸¤ä¸ªç›¸é‚»çš„åˆ‡ç‰‡ä¸­ã€‚ä½ çš„ä»»åŠ¡æ˜¯é€ä¸ªå¤„ç†è¿™äº›åˆ‡ç‰‡ï¼Œå¹¶ä»¥æœ€é«˜çš„ä¿çœŸåº¦è¿˜åŸè¯•é¢˜ã€‚

**æ ¸å¿ƒä»»åŠ¡**: ç»™æˆ‘ä¸¤ä¸ªæ–‡æœ¬åˆ‡ç‰‡ï¼š`[current_slice]`ï¼ˆå½“å‰å¤„ç†çš„åˆ‡ç‰‡ï¼‰å’Œ `[next_slice]`ï¼ˆç´§éšå…¶åçš„åˆ‡ç‰‡ï¼‰ï¼Œè¯·ä½ ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹é€»è¾‘è¯†åˆ«å¹¶æ ¼å¼åŒ–`[current_slice]`ä¸­çš„æ‰€æœ‰è¯•é¢˜ã€‚

**è¾“å…¥**:

1. `[current_slice]`: å½“å‰éœ€è¦å¤„ç†çš„ä¸»è¦æ–‡æœ¬å—ã€‚
2. `[next_slice]`: ä¸‹ä¸€ä¸ªæ–‡æœ¬å—ï¼Œä»…ç”¨ä½œ"å‰ç»ç¼“å†²åŒº"æ¥è¡¥å…¨`[current_slice]`æœ«å°¾çš„æ–­è£‚è¯•é¢˜ã€‚

        **å¤„ç†é€»è¾‘ä¸è§„åˆ™**:

        1. **è¯†åˆ«èµ·ç‚¹ (å¿½ç•¥å·²å¤„ç†çš„å¤´éƒ¨)**:
           * åœ¨`[current_slice]`çš„å¼€å¤´ï¼Œåˆ¤æ–­å…¶å†…å®¹æ˜¯å¦æ˜¯ä¸€ä¸ªå®Œæ•´è¯•é¢˜çš„å¼€å§‹ã€‚ä¸€ä¸ªè¯•é¢˜çš„æ˜ç¡®å¼€å§‹æ ‡å¿—åŒ…æ‹¬ï¼š
             1) **ä»¥æ•°å­—å¼€å¤´çš„è¡Œ**ï¼ˆå¦‚ `1. `ã€`2ï¼`ã€`3ï¼ˆéš¾åº¦...ï¼‰`ç­‰ï¼‰ï¼›
             2) **ä¸ä»¥æ•°å­—å¼€å¤´**ä½†æœ¬è¡Œæˆ–ä¸‹ä¸€è¡ŒåŒ…å«éš¾åº¦æ ‡è¯†ï¼ˆå¦‚`(éš¾åº¦: x)`ã€`ï¼ˆéš¾åº¦xï¼‰`ã€`ï¼ˆéš¾åº¦ï¼šxï¼‰`ç­‰ï¼‰çš„å®Œæ•´é—®å¥ï¼›
             3) ä¸€ä¸ªå®Œæ•´é—®å¥åï¼Œ**ç´§éšå…¶å**å‡ºç° `å‚è€ƒç­”æ¡ˆ`/`å‚è€ƒç­”æ¡ˆè¦ç‚¹`/`ã€å‚è€ƒç­”æ¡ˆã€‘` æç¤ºã€‚
           * å¦‚æœ`[current_slice]`çš„å¼€å¤´æ˜æ˜¾å±äºä¸Šä¸€é¢˜çš„**ç­”æ¡ˆä¸­é—´éƒ¨åˆ†**ï¼ˆå¦‚ä»¥é¡¹ç›®ç¬¦å·`â€¢`/`-`å¼€å¤´çš„ç­”æ¡ˆè¦ç‚¹ç­‰ï¼‰ï¼Œä½ **å¿…é¡»å¿½ç•¥**è¿™äº›å†…å®¹ï¼Œç›´åˆ°æ‰¾åˆ°ä¸Šè¿°ä»»ä¸€**æ˜ç¡®çš„è¯•é¢˜å¼€å§‹æ ‡å¿—**ä¸ºæ­¢ã€‚

2. **é¡ºåºå¤„ç†ä¸è¯†åˆ«**:
   * ä»ä½ æ‰¾åˆ°çš„ç¬¬ä¸€ä¸ªè¯•é¢˜å¼€å§‹æ ‡å¿—èµ·ï¼Œé¡ºåºå‘ä¸‹è§£æ`[current_slice]`ä¸­çš„æ¯ä¸€é“è¯•é¢˜ã€‚

        3. **å¤„ç†æœ«å°¾çš„æ–­è£‚è¯•é¢˜ (å‰ç»æ‹¼æ¥)**:
   * å½“ä½ è§£æåˆ°`[current_slice]`ä¸­çš„**æœ€åä¸€ä¸ªè¯•é¢˜**æ—¶ï¼Œæ£€æŸ¥å®ƒæ˜¯å¦åœ¨`[current_slice]`çš„æœ«å°¾è¢«æˆªæ–­ã€‚
   * å¦‚æœè¢«æˆªæ–­ï¼Œä½ **å¿…é¡»**æŸ¥çœ‹`[next_slice]`çš„å¼€å¤´éƒ¨åˆ†ï¼Œå¹¶ä»ä¸­**é€å­—å¤åˆ¶**å†…å®¹ï¼Œç›´åˆ°å°†è¿™é“è¢«æˆªæ–­çš„è¯•é¢˜è¡¥å……å®Œæ•´ä¸ºæ­¢ã€‚
           * **æ‹¼æ¥ç•Œé™**ï¼šä»`[next_slice]`ä¸­å¤åˆ¶å†…å®¹çš„ç»ˆç‚¹æ˜¯è¿™é“é¢˜çš„ç»“å°¾ã€‚ä¸€æ—¦é‡åˆ°ä¸‹ä¸€ä¸ª**æ˜ç¡®çš„è¯•é¢˜å¼€å§‹æ ‡å¿—**ï¼ˆå‚è§ä¸Šæ–‡ä¸‰ç±»å¼€å§‹æ ‡å¿—ï¼Œè€Œä¸ä»…æ˜¯ä»¥æ•°å­—å¼€å¤´ï¼‰ï¼Œå°±åº”ç«‹å³åœæ­¢ã€‚

        4. **ä¸¥æ ¼é™å®šå¤„ç†èŒƒå›´ (é˜²æ­¢è¶Šç•Œ)**:
   * ä½ å¯¹`[next_slice]`çš„ä½¿ç”¨**ä»…é™äº**è¡¥å…¨`[current_slice]`æœ«å°¾çš„é‚£ä¸€é“æ–­è£‚è¯•é¢˜ã€‚
           * **ç»å¯¹ä¸èƒ½**å¤„ç†ä»»ä½•åœ¨`[next_slice]`ä¸­å®Œæ•´å¼€å§‹çš„æ–°è¯•é¢˜ï¼ˆæ— è®ºæ˜¯å¦ä»¥æ•°å­—å¼€å¤´ï¼Œåªè¦æ»¡è¶³ä¸Šè¿°å¼€å§‹æ ‡å¿—ï¼Œéƒ½è§†ä¸ºæ–°è¯•é¢˜ï¼Œåº”ç•™å¾…ä¸‹ä¸€è½®å¤„ç†ï¼‰ã€‚

5. **è¾“å‡ºä¸ç¼–å·**:
   * ä»…è¾“å‡ºä½ åœ¨æœ¬è½®ï¼ˆå³åœ¨`[current_slice]`ä¸­è¯†åˆ«å¹¶å®Œæˆçš„ï¼‰æ‰€æœ‰è¯•é¢˜ã€‚
   * å¯¹è¾“å‡ºçš„è¯•é¢˜è¿›è¡Œ**è¿ç»­ç¼–å·**ï¼Œä»`### è¯•é¢˜ 1`å¼€å§‹ã€‚

**å­—æ®µæå–ä¸æ ¼å¼åŒ–æ ‡å‡† (ã€é«˜ä¿çœŸã€‘æ ¸å¿ƒæŒ‡ä»¤)**:

* **é¢˜å‹**: æ­¤å­—æ®µå†…å®¹å›ºå®šä¸º `ç®€ç­”I`ã€‚

* **éš¾åº¦**:
  * å¯ä»é¢˜å¹²ä¸­æå–éš¾åº¦ä¿¡æ¯ï¼Œå¦‚ `(éš¾åº¦: 4)` æˆ– `(éš¾åº¦5)`ï¼›è‹¥æ— åˆ™è¾“å‡º `æœªæä¾›`ã€‚

* **é¢˜å¹²**:
  * **ã€æœ€é«˜ä¼˜å…ˆçº§æŒ‡ä»¤ã€‘**: ä½ å¿…é¡»å¯¹é¢˜å¹²è¿›è¡Œ**å®Œå…¨é€å­—ï¼ˆverbatimï¼‰çš„å¤åˆ¶**ã€‚ç¦æ­¢æ€»ç»“ã€æ”¹å†™æˆ–æ¶¦è‰²ã€‚
  * å…è®¸çš„å”¯ä¸€ä¿®æ”¹ï¼šä»ä¸­ç§»é™¤éš¾åº¦æ ‡è¯†ï¼ˆå¦‚ ` (éš¾åº¦: 4) `ï¼‰ã€‚

* **é€‰æ‹©é¡¹**: å›ºå®šè¾“å‡º `æ— `ã€‚

* **ç­”æ¡ˆ**:
  * **ã€æœ€é«˜ä¼˜å…ˆçº§æŒ‡ä»¤ã€‘**: å¯¹`å‚è€ƒç­”æ¡ˆ:` æˆ– `ç­”æ¡ˆ:`ä¹‹åçš„å†…å®¹**å®Œå…¨é€å­—å¤åˆ¶**ï¼Œä¿ç•™åŸæ–‡çš„æ‰€æœ‰ç¬¦å·ã€æ¢è¡Œä¸ç¼©è¿›ã€‚

**è¾“å‡ºæ ¼å¼**:
æ¯ä¸ªè¯•é¢˜ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š

```
### è¯•é¢˜ X

#### é¢˜å‹
ç®€ç­”I

#### éš¾åº¦
[éš¾åº¦ä¿¡æ¯]

#### é¢˜å¹²
[å®Œå…¨ä¿çœŸçš„é¢˜å¹²å†…å®¹]

#### é€‰æ‹©é¡¹
æ— 

#### ç­”æ¡ˆ
[å®Œå…¨ä¿çœŸçš„ç­”æ¡ˆå†…å®¹]

=== é¢˜ç›®åˆ†éš”ç¬¦ ===
```

**å½“å‰å¤„ç†çš„æ–‡æœ¬å— [current_slice]**:
```
{current_slice}
```

**ä¸‹ä¸€ä¸ªæ–‡æœ¬å— [next_slice]** (ä»…ç”¨äºè¡¥å…¨æ–­è£‚è¯•é¢˜):
```
{next_slice}
```

è¯·å¼€å§‹å¤„ç†ï¼Œä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°è§„åˆ™è¿›è¡Œç»å¯¹ä¿çœŸçš„è¯•é¢˜è¯†åˆ«å’Œæ ¼å¼åŒ–ã€‚"""

        return prompt

    def call_ai_standardization(self, prompt: str) -> Optional[str]:
        return call_openai_with_retries(
            client=self.client,
            model=self.model,
            prompt=prompt,
            max_retries=self.config["max_retries"],
            temperature=0.1,
        )

    def parse_standardized_result(self, ai_response: str) -> List[str]:
        return split_questions_by_separator(ai_response)

    def save_chunk_results(self, chunk_index: int, questions: List[str], output_dir: str):
        save_markdown_chunk_result(
            chunk_index=chunk_index,
            questions=questions,
            output_dir=output_dir,
            question_type_name=self.get_question_type_name(),
        )

    def save_original_chunk(self, chunk_index: int, chunk1: List[str], chunk2: Optional[List[str]], output_dir: str):
        save_original_chunk_file(
            chunk_index=chunk_index,
            chunk1=chunk1,
            output_dir=output_dir,
        )

    def standardize_file(self, input_file: str, output_dir: str = None) -> Dict:
        if output_dir is None:
            base_dir = os.path.dirname(input_file)
            output_dir = os.path.join(base_dir, f"{self.get_question_type_name()}_standardized")

        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        print(f"ğŸš€ å¼€å§‹æ ‡å‡†åŒ– {self.get_question_type_name()}")
        print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {input_file}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

        if self.config["preserve_original"]:
            backup_file = os.path.join(output_dir, "original_backup.md")
            os.system(f'cp "{input_file}" "{backup_file}"')
            print(f"ğŸ’¾ åŸæ–‡ä»¶å·²å¤‡ä»½åˆ°: {backup_file}")

        print(f"ğŸ”ª æ­£åœ¨åˆ‡åˆ†æ–‡ä»¶...")
        chunks = self.chunk_file(input_file)
        print(f"ğŸ“ æ–‡ä»¶å·²åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªå—")

        if self.config["preserve_original"]:
            print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜åŸå§‹åˆ†å—æ–‡ä»¶...")
            for i, (chunk1, chunk2) in enumerate(chunks, 1):
                self.save_original_chunk(i, chunk1, chunk2, output_dir)

        total_questions = 0
        for i, (chunk1, chunk2) in enumerate(chunks, 1):
            print(f"\nğŸ”„ å¤„ç† Chunk {i}/{len(chunks)}")
            prompt = self.create_standardization_prompt(chunk1, chunk2)
            ai_response = self.call_ai_standardization(prompt)
            if ai_response is None:
                print(f"âŒ Chunk {i} AIè°ƒç”¨å¤±è´¥ï¼Œè·³è¿‡")
                continue
            questions = self.parse_standardized_result(ai_response)
            self.save_chunk_results(i, questions, output_dir)
            total_questions += len(questions)

        quality_stats = {
            "question_type": self.get_question_type_name(),
            "total_chunks": len(chunks),
            "total_questions": total_questions,
            "processing_time": datetime.now().isoformat(),
            "config": self.config,
        }

        stats_file = os.path.join(output_dir, "quality_stats.json")
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(quality_stats, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ‰ {self.get_question_type_name()} æ ‡å‡†åŒ–å®Œæˆï¼")
        print(f"ğŸ“Š æ€»è®¡å¤„ç†: {len(chunks)} ä¸ªå—, {total_questions} é“é¢˜ç›®")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")

        return quality_stats

    def extract_questions_from_standardized_files(self, standardized_dir: str) -> List[Dict]:
        questions: List[Dict] = []
        chunk_files = list_standardized_chunk_files(standardized_dir)
        print(f"ğŸ” æ‰¾åˆ° {len(chunk_files)} ä¸ªæ ‡å‡†åŒ–æ–‡ä»¶")

        for chunk_file in chunk_files:
            print(f"ğŸ“– æ­£åœ¨å¤„ç†: {os.path.basename(chunk_file)}")
            with open(chunk_file, 'r', encoding='utf-8') as f:
                content = f.read()

            blocks = extract_codeblocks_from_markdown(content)
            for block in blocks:
                question_data = self.parse_question_block(block)
                if question_data:
                    questions.append(question_data)

        print(f"âœ… æ€»å…±æå– {len(questions)} é“é¢˜ç›®")
        return questions

    def parse_question_block(self, block: str) -> Optional[Dict]:
        try:
            patterns = {
                'question_number': r'### è¯•é¢˜ (\d+)',
                'question_type': r'#### é¢˜å‹\n(.*?)(?=\n|$)',
                'difficulty': r'#### éš¾åº¦\n(.*?)(?=\n|$)',
                'question_stem': r'#### é¢˜å¹²\n(.*?)(?=#### é€‰æ‹©é¡¹)',
                'options': r'#### é€‰æ‹©é¡¹\n(.*?)(?=#### ç­”æ¡ˆ)',
                'answer': r'#### ç­”æ¡ˆ\n(.*?)(?=\n=== é¢˜ç›®åˆ†éš”ç¬¦ ===|$)'
            }

            extracted: Dict[str, str] = {}
            for field, pattern in patterns.items():
                match = re.search(pattern, block, re.DOTALL)
                extracted[field] = match.group(1).strip() if match else ""

            if not extracted.get('question_stem') or not extracted.get('answer'):
                print(f"âš ï¸  è·³è¿‡ä¸å®Œæ•´çš„é¢˜ç›®: {extracted.get('question_number', 'æœªçŸ¥')}")
                return None

            return {
                'code': '',
                'question_type': extracted['question_type'] or 'ç®€ç­”I',
                'difficulty': extracted['difficulty'] or 'æœªæä¾›',
                'question_stem': extracted['question_stem'],
                'option_A': '',
                'option_B': '',
                'option_C': '',
                'option_D': '',
                'option_E': '',
                'answer': extracted['answer'],
                'score': '',
                'consistency': '',
            }
        except Exception as exc:
            print(f"âŒ è§£æé¢˜ç›®å—æ—¶å‡ºé”™: {exc}")
            return None

    def create_excel_file(self, questions: List[Dict], output_path: str):
        headers = [
            'ä»£ç ', 'é¢˜å‹', 'éš¾åº¦', 'é¢˜å¹²',
            'é€‰æ‹©é¡¹A', 'é€‰æ‹©é¡¹B', 'é€‰æ‹©é¡¹C', 'é€‰æ‹©é¡¹D', 'é€‰æ‹©é¡¹E',
            'ç­”æ¡ˆ', 'åˆ†æ•°', 'é¢˜ç›®ä¸€è‡´æ€§'
        ]

        rows: List[List[str]] = []
        for q in questions:
            rows.append([
                q.get('code', ''),
                q.get('question_type', ''),
                q.get('difficulty', ''),
                q.get('question_stem', ''),
                q.get('option_A', ''),
                q.get('option_B', ''),
                q.get('option_C', ''),
                q.get('option_D', ''),
                q.get('option_E', ''),
                q.get('answer', ''),
                q.get('score', ''),
                q.get('consistency', ''),
            ])

        write_excel(headers=headers, rows=rows, sheet_title="ç®€ç­”é¢˜", output_path=output_path)

    def process_standardized_to_excel(self, standardized_dir: str, output_dir: str = None) -> Optional[str]:
        if output_dir is None:
            output_dir = standardized_dir

        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        print("ğŸš€ å¼€å§‹å¤„ç†æ ‡å‡†åŒ–æ–‡ä»¶ç”ŸæˆExcel")
        print(f"ğŸ“ æ ‡å‡†åŒ–æ–‡ä»¶ç›®å½•: {standardized_dir}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

        questions = self.extract_questions_from_standardized_files(standardized_dir)
        if not questions:
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆé¢˜ç›®ï¼Œè·³è¿‡Excelç”Ÿæˆ")
            return None

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        excel_filename = f"ç®€ç­”é¢˜_{timestamp}.xlsx"
        excel_path = os.path.join(output_dir, excel_filename)

        self.create_excel_file(questions, excel_path)

        stats = {
            "question_type": self.get_question_type_name(),
            "total_questions": len(questions),
            "excel_file": excel_path,
            "processing_time": datetime.now().isoformat(),
        }
        stats_file = os.path.join(output_dir, f"excel_generation_stats_{timestamp}.json")
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)

        print("ğŸ‰ Excelç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡: {stats}")
        return excel_path


def main():
    """æ‰§è¡Œç®€ç­”é¢˜æ ‡å‡†åŒ–å¹¶å¯¼å‡ºExcel"""
    import os
    from config import Config

    config = Config.get_openai_config()
    api_key = config['api_key']
    api_base = config.get('api_base', 'https://api.openai.com/v1')
    model = config.get('model', 'gpt-4o')

    standardizer = ShortAnswerStandardizer(
        api_key=api_key,
        api_base=api_base,
        model=model,
    )

    # å¯è°ƒæ•´chunkå¤§å°
    standardizer.config["lines_per_chunk"] = 120

    input_file = "question_processing_ã€Šæ•°æ®å®‰å…¨ç®¡ç†å‘˜é¢˜åº“ã€‹ï¼ˆå®¢è§‚é¢˜ï¼‰-20250713ï¼ˆæäº¤ç‰ˆï¼‰/question_types/short_answer.md"

    if os.path.exists(input_file):
        # æ ‡å‡†åŒ–
        result = standardizer.standardize_file(input_file)
        print(f"\nğŸ‰ æ ‡å‡†åŒ–å®Œæˆï¼")
        print(f"ğŸ“Š å¤„ç†ç»“æœ: {result}")

        # å¯¼å‡ºExcel
        standardized_dir = os.path.join(
            os.path.dirname(input_file),
            f"{standardizer.get_question_type_name()}_standardized"
        )
        if os.path.exists(standardized_dir):
            excel_path = standardizer.process_standardized_to_excel(standardized_dir)
            if excel_path:
                print(f"\nâœ… Excelæ–‡ä»¶ç”Ÿæˆå®Œæˆ: {excel_path}")
            else:
                print(f"\nâŒ Excelæ–‡ä»¶ç”Ÿæˆå¤±è´¥")
        else:
            print(f"âŒ æ ‡å‡†åŒ–ç›®å½•ä¸å­˜åœ¨: {standardized_dir}")
    else:
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")


if __name__ == "__main__":
    main()


